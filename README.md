# Pre-trained-Models
Pre-trained Models for Natural Language Processing

### static word embedding
- word2vec: [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)

### pre-trained encoders
#### LSTM
- ELMO [Deep Contextualized Word Representations](https://www.aclweb.org/anthology/N18-1202.pdf)

#### Transformer Encoder
- BERT [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

#### Transformer Decoder
- GPT-1 [Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)

#### Transformer
- MASS [Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/pdf/1905.02450.pdf)
- T5 [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)

#### Transformer-XL
- XLNet
