{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## an example with word2vec based on gensim\n",
    "This notebook introduces the example from the gensim word2vec source code.\n",
    "\n",
    "**gensim version 4.0.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "- 输入: 句子分词的结果\n",
    "- 输出: 对应每个词的向量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[computer] vector:  [-0.00515774 -0.00667028 -0.0077791   0.00831315 -0.00198292 -0.00685696\n",
      " -0.0041556   0.00514562 -0.00286997 -0.00375075  0.0016219  -0.0027771\n",
      " -0.00158482  0.0010748  -0.00297881  0.00852176  0.00391207 -0.00996176\n",
      "  0.00626142 -0.00675622  0.00076966  0.00440552 -0.00510486 -0.00211128\n",
      "  0.00809783 -0.00424503 -0.00763848  0.00926061 -0.00215612 -0.00472081\n",
      "  0.00857329  0.00428458  0.0043261   0.00928722 -0.00845554  0.00525685\n",
      "  0.00203994  0.0041895   0.00169839  0.00446543  0.00448759  0.0061063\n",
      " -0.00320303 -0.00457706 -0.00042664  0.00253447 -0.00326412  0.00605948\n",
      "  0.00415534  0.00776685  0.00257002  0.00811904 -0.00138761  0.00808028\n",
      "  0.0037181  -0.00804967 -0.00393476 -0.0024726   0.00489447 -0.00087241\n",
      " -0.00283173  0.00783599  0.00932561 -0.0016154  -0.00516075 -0.00470313\n",
      " -0.00484746 -0.00960562  0.00137242 -0.00422615  0.00252744  0.00561612\n",
      " -0.00406709 -0.00959937  0.00154715 -0.00670207  0.0024959  -0.00378173\n",
      "  0.00708048  0.00064041  0.00356198 -0.00273993 -0.00171105  0.00765502\n",
      "  0.00140809 -0.00585215 -0.00783678  0.00123304  0.00645651  0.00555797\n",
      " -0.00897966  0.00859466  0.00404815  0.00747178  0.00974917 -0.0072917\n",
      " -0.00904259  0.0058377   0.00939395  0.00350795]\n",
      "\n",
      "\n",
      "similar words:  [('system', 0.21617144346237183), ('survey', 0.044689204543828964), ('interface', 0.01520337164402008), ('time', 0.0019510634010657668), ('trees', -0.032843153923749924), ('human', -0.0742427185177803), ('response', -0.09317589551210403), ('graph', -0.09575346857309341), ('eps', -0.10513806343078613), ('user', -0.16911624372005463)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# dataset\n",
    "common_texts = [\n",
    "    ['human', 'interface', 'computer'],\n",
    "    ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "    ['eps', 'user', 'interface', 'system'],\n",
    "    ['system', 'human', 'system', 'eps'],\n",
    "    ['user', 'response', 'time'],\n",
    "    ['trees'],\n",
    "    ['graph', 'trees'],\n",
    "    ['graph', 'minors', 'trees'],\n",
    "    ['graph', 'minors', 'survey']\n",
    "]\n",
    "\n",
    "# construct model\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# obtain word vector example\n",
    "vector = model.wv['computer']  # get numpy vector of a word\n",
    "sims = model.wv.most_similar('computer', topn=10)  # get other similar words\n",
    "\n",
    "print(\"[computer] vector: \", vector)\n",
    "print('\\n')\n",
    "print(\"similar words: \", sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramenters\n",
    "\n",
    "Word2Vec类初始化函数\n",
    "```python\n",
    "def __init__(\n",
    "        self, sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5,\n",
    "        max_vocab_size=None, sample=1e-3, seed=1, workers=3, min_alpha=0.0001,\n",
    "        sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=hash, epochs=5, null_word=0,\n",
    "        trim_rule=None, sorted_vocab=1, batch_words=MAX_WORDS_IN_BATCH, compute_loss=False, callbacks=(),\n",
    "        comment=None, max_final_vocab=None,\n",
    "    )\n",
    "```\n",
    "\n",
    "- sentences: 待输入分词结果\n",
    "- corpus_file: 语料库文件路径, 格式要求详见 class:`~gensim.models.word2vec.LineSentence`\n",
    "- vector_size: word vector的维度\n",
    "- alpha: 学习率初始化\n",
    "- window: 当前词与待预测词之间的最大距离\n",
    "- min_count: 忽略频率小于此值的单词\n",
    "- sg: (可选, 0或1) 0表示CBOW，1表示skip-gram\n",
    "- hs: (可选, 0或1) 1表示模型训练时使用分层softmax, 0表示(且negative参数不为0),表示使用negative sampling\n",
    "- negative: 负样本数量(noise word)\n",
    "- nx_exponent: negative sampling分布指数, 值为1表示根据频率采样, 值为0表示对所有单词均等采样, 值为负值表示采样低频单词多于高频单词\n",
    "- cbow_mean: 如果为0, 使用上下文词向量的和; 为1, 使用上下文词向量均值(仅适用cbow)\n",
    "- min_alpha: 学习速率下降的最低值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### scan_vocab 单词初始化\n",
    "\n",
    "```python\n",
    "    def _scan_vocab(self, sentences, progress_per, trim_rule):\n",
    "        sentence_no = -1\n",
    "        total_words = 0\n",
    "        min_reduce = 1\n",
    "        vocab = defaultdict(int)\n",
    "        checked_string_types = 0\n",
    "        for sentence_no, sentence in enumerate(sentences):\n",
    "            if not checked_string_types:\n",
    "                if isinstance(sentence, str):\n",
    "                    logger.warning(\n",
    "                        \"Each 'sentences' item should be a list of words (usually unicode strings). \"\n",
    "                        \"First item here is instead plain %s.\",\n",
    "                        type(sentence),\n",
    "                    )\n",
    "                checked_string_types += 1\n",
    "            if sentence_no % progress_per == 0:\n",
    "                logger.info(\n",
    "                    \"PROGRESS: at sentence #%i, processed %i words, keeping %i word types\",\n",
    "                    sentence_no, total_words, len(vocab)\n",
    "                )\n",
    "            for word in sentence:\n",
    "                vocab[word] += 1   # 记录每个词出现的次数\n",
    "            total_words += len(sentence)  # 记录出现的单词总数\n",
    "\n",
    "            if self.max_vocab_size and len(vocab) > self.max_vocab_size:\n",
    "                utils.prune_vocab(vocab, min_reduce, trim_rule=trim_rule)\n",
    "                min_reduce += 1\n",
    "\n",
    "        corpus_count = sentence_no + 1\n",
    "        self.raw_vocab = vocab  # 字典形式, 记录每个词出现的次数\n",
    "        return total_words, corpus_count  # total words单词总数, corpus_count 句子数\n",
    "```\n",
    "\n",
    "- 输入: sentences分词结果\n",
    "- 输出: total words单词总数, corpus_count文档数(sentence_length + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### prepare_vocab 采样控制\n",
    "\n",
    "##### 函数头\n",
    "```python\n",
    "    def prepare_vocab(\n",
    "            self, update=False, keep_raw_vocab=False, trim_rule=None,\n",
    "            min_count=None, sample=None, dry_run=False,\n",
    "        )\n",
    "```\n",
    "parameters:\n",
    "- min_count: 忽略频率小于此值的单词\n",
    "- sample: 控制more-frequent words的下采样\n",
    "\n",
    "##### 加载新的词汇表\n",
    "```python\n",
    "       if not update:\n",
    "            logger.info(\"Creating a fresh vocabulary\")\n",
    "            retain_total, retain_words = 0, []\n",
    "            # Discard words less-frequent than min_count\n",
    "            if not dry_run:\n",
    "                self.wv.index_to_key = []\n",
    "                # make stored settings match these applied settings\n",
    "                self.min_count = min_count\n",
    "                self.sample = sample\n",
    "                self.wv.key_to_index = {}\n",
    "\n",
    "            for word, v in self.raw_vocab.items():\n",
    "                if keep_vocab_item(word, v, self.effective_min_count, trim_rule=trim_rule):\n",
    "                    retain_words.append(word)\n",
    "                    retain_total += v\n",
    "                    if not dry_run:\n",
    "                        self.wv.key_to_index[word] = len(self.wv.index_to_key)\n",
    "                        self.wv.index_to_key.append(word)\n",
    "                else:\n",
    "                    drop_unique += 1\n",
    "                    drop_total += v\n",
    "```\n",
    "- self.rae_vocab为**scan_vocab**中保留的单词表dict\n",
    "- keep_vocab_item()判断单词是否需要被忽略/丢弃\n",
    "\n",
    "##### 利用新的词汇更新模型\n",
    "```python\n",
    "        else:\n",
    "            logger.info(\"Updating model with new vocabulary\")\n",
    "            new_total = pre_exist_total = 0\n",
    "            new_words = []\n",
    "            pre_exist_words = []\n",
    "            for word, v in self.raw_vocab.items():\n",
    "                if keep_vocab_item(word, v, self.effective_min_count, trim_rule=trim_rule):\n",
    "                    if self.wv.has_index_for(word):\n",
    "                        pre_exist_words.append(word)\n",
    "                        pre_exist_total += v\n",
    "                        if not dry_run:\n",
    "                            pass\n",
    "                    else:\n",
    "                        new_words.append(word)\n",
    "                        new_total += v\n",
    "                        if not dry_run:\n",
    "                            self.wv.key_to_index[word] = len(self.wv.index_to_key)\n",
    "                            self.wv.index_to_key.append(word)\n",
    "                else:\n",
    "                    drop_unique += 1\n",
    "                    drop_total += v\n",
    "```\n",
    "- 同上, 最终保留drop_unique(丢弃的单词数,不重复), drop_total(丢弃的单词总数)结果以及retain_words, retain_total\n",
    "\n",
    "##### 计算采样阈值\n",
    "```python\n",
    "# Precalculate each vocabulary item's threshold for sampling\n",
    "        if not sample:\n",
    "            # no words downsampled\n",
    "            threshold_count = retain_total\n",
    "        elif sample < 1.0:\n",
    "            # traditional meaning: set parameter as proportion of total\n",
    "            threshold_count = sample * retain_total\n",
    "        else:\n",
    "            # new shorthand: sample >= 1 means downsample all words with higher count than sample\n",
    "            threshold_count = int(sample * (3 + np.sqrt(5)) / 2)\n",
    "\n",
    "        downsample_total, downsample_unique = 0, 0\n",
    "        for w in retain_words:\n",
    "            v = self.raw_vocab[w]\n",
    "            word_probability = (np.sqrt(v / threshold_count) + 1) * (threshold_count / v)\n",
    "            if word_probability < 1.0:\n",
    "                downsample_unique += 1\n",
    "                downsample_total += word_probability * v\n",
    "            else:\n",
    "                word_probability = 1.0\n",
    "                downsample_total += v\n",
    "            if not dry_run:\n",
    "                self.wv.set_vecattr(w, 'sample_int', np.uint32(word_probability * (2**32 - 1)))\n",
    "```\n",
    "- 采样公式计算: `word_probability = (np.sqrt(v / threshold_count) + 1) * (threshold_count / v)`,其中threshold表示该词频数*采样率\n",
    "\n",
    "##### 根据最终词汇表选择建立表格make_cum_table和构建二叉树create_binary_tree\n",
    "```python\n",
    "    # return from each step: words-affected, resulting-corpus-size, extra memory estimates\n",
    "        if self.sorted_vocab and not update:\n",
    "            self.wv.sort_by_descending_frequency()\n",
    "\n",
    "        if self.hs:\n",
    "            # add info about each word's Huffman encoding\n",
    "            self.create_binary_tree()\n",
    "        if self.negative:\n",
    "            # build the table for drawing random words (for negative sampling)\n",
    "            self.make_cum_table()\n",
    "```\n",
    "- hs和negative分别对应两种构建词表方法"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### create_binary_tree() 构建二叉树\n",
    "\n",
    "```python\n",
    "    # constructing a huffman tree from %i words\n",
    "    heap = _build_heap(wv)\n",
    "    if not heap:\n",
    "        #\n",
    "        # TODO: how can we end up with an empty heap?\n",
    "        #\n",
    "        logger.info(\"built huffman tree with maximum node depth 0\")\n",
    "        return\n",
    "\n",
    "    # recurse over the tree, assigning a binary code to each vocabulary word\n",
    "    max_depth = 0\n",
    "    stack = [(heap[0], [], [])]\n",
    "    while stack:\n",
    "        node, codes, points = stack.pop()\n",
    "        if node[1] < len(wv):  # node[1] = index\n",
    "            # leaf node => store its path from the root\n",
    "            k = node[1]\n",
    "            wv.set_vecattr(k, 'code', codes)\n",
    "            wv.set_vecattr(k, 'point', points)\n",
    "            # node.code, node.point = codes, points\n",
    "            max_depth = max(len(codes), max_depth)\n",
    "        else:\n",
    "            # inner node => continue recursion\n",
    "            points = np.array(list(points) + [node.index - len(wv)], dtype=np.uint32)\n",
    "            stack.append((node.left, np.array(list(codes) + [0], dtype=np.uint8), points))\n",
    "            stack.append((node.right, np.array(list(codes) + [1], dtype=np.uint8), points))\n",
    "```\n",
    "- 哈夫曼树, 带权路径长度最短的树\n",
    "- 保证词频高的单词路径短,词频相对较低的单词的路径长,可以减少计算量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### make_cum_table() 负采样\n",
    "\n",
    "```python\n",
    "    def make_cum_table(self, domain=2**31 - 1):\n",
    "        vocab_size = len(self.wv.index_to_key)\n",
    "        self.cum_table = np.zeros(vocab_size, dtype=np.uint32)\n",
    "        # compute sum of all power (Z in paper)\n",
    "        train_words_pow = 0.0\n",
    "        for word_index in range(vocab_size):\n",
    "            count = self.wv.get_vecattr(word_index, 'count')\n",
    "            train_words_pow += count**self.ns_exponent\n",
    "        cumulative = 0.0\n",
    "        for word_index in range(vocab_size):\n",
    "            count = self.wv.get_vecattr(word_index, 'count')\n",
    "            cumulative += count**self.ns_exponent\n",
    "            self.cum_table[word_index] = round(cumulative / train_words_pow * domain)\n",
    "        if len(self.cum_table) > 0:\n",
    "            assert self.cum_table[-1] == domain\n",
    "```\n",
    "- 负采样原理:\n",
    "    - 将长度为1的线段分为$V$份($V$为词汇表大小), 每份对应词汇表中的一个词(此时,词汇表中的词按频次排序)\n",
    "    - 高频词对应的线段长, 低频次对应的线段短(每个词$w$的线段长度计算公式如下)\n",
    "\n",
    "        $$len(w)=\\frac{count(w)^{ns\\_exponent}}{\\sum_{u \\in vocab}count(u)^{ns\\_exponent}}$$\n",
    "\n",
    "    - 采样前, 将这段长度为1的线段分成$M$等份($M>>N$), 并与之前的线段进行映射(可以保证没歌词对应的线段都会划分为对应的小块)\n",
    "    - 采样时, 从$M$个位置中采样出 neg 个位置,此时采样到的每个位置对应线段所属的词就是负样本词\n",
    "- parameters\n",
    "    - domain 表示均分的份数,即对应$M$\n",
    "    - nx_exponent 表示指数幂, 默认为0.75\n",
    "    - cumulative 表示公式中的分子\n",
    "    - train_words_pow 表示公式中的分母\n",
    "- 代码实际计算的是在这条线段中,所占据部分的右端index, 因此计算公式为`round(cumulative / train_words_pow * domain)`\n",
    "- 输出负采样所需词对应占比表"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### train_batch_sg() skip_gram模型 & train_batch_cbow CBOW模型\n",
    "gensim使用的C版本的模型\n",
    "```python\n",
    "from gensim.models.word2vec_inner import (  # noqa: F401\n",
    "        train_batch_sg,\n",
    "        train_batch_cbow,\n",
    "        score_sentence_sg,\n",
    "        score_sentence_cbow,\n",
    "        MAX_WORDS_IN_BATCH,\n",
    "        FAST_VERSION,\n",
    "    )\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanfordcorenlp",
   "language": "python",
   "name": "stanfordcorenlp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}